# -*- coding: utf-8 -*-
"""RAG QA System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JyUAIkH4dTlhoQbqrlQiSKzi-kguKzNd
"""

# Install Required Libraries
!pip install faiss-cpu sentence-transformers pymupdf langchain gradio transformers

# Import Libraries
from google.colab import files
import fitz  # PyMuPDF
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import pipeline
import gradio as gr

# Step 1: Upload PDF
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

# Step 2: Parse and Chunk PDF
doc = fitz.open(file_name)
text = ""
for page in doc:
    text += page.get_text()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_text(text)
    print(f"Document split into {len(chunks)} chunks.")

    # Step 3: Create Embeddings and Build FAISS Index
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings))
    print(f"FAISS index created with {index.ntotal} chunks.")

    # Step 4: Build Retriever
    def retrieve_top_k(query, k=3):
        query_embedding = model.encode([query])
        distances, indices = index.search(np.array(query_embedding), k)
        results = [chunks[i] for i in indices[0]]
        return results


    # Step 5: Load Local Free Answer Generator (Upgrade Model)
    generator = pipeline('text2text-generation', model='google/flan-t5-base')

    def generate_answer(question, context):
        prompt = f"""You are an expert AI assistant. Using the provided context, give a detailed, step-by-step, and fully explained answer to the question.
        Your answer should be long, clear, and easy to understand, as if you are teaching someone.

        Context:
        {context}

        Question:
        {question}

    Step-by-Step Detailed Answer:"""

        result = generator(prompt, max_length=700, do_sample=False)
        answer = result[0]['generated_text']
        return answer




























                                    # Step 6: Full RAG QA Pipeline
    def answer_question(question):
        relevant_chunks = retrieve_top_k(question)
        context = "\n\n".join(relevant_chunks)
        answer = generate_answer(question, context)
        return answer

                                                    # Step 7: Gradio UI (Input Box Will Appear Automatically)
    iface = gr.Interface(
        fn=answer_question,
        inputs=gr.Textbox(lines=2, placeholder="Ask your question here..."),
        outputs="text",
        title="RAG QA System (Fully Local, Free)",
        description="Ask any question based on the uploaded PDF.",
    )

    iface.launch(share=True)